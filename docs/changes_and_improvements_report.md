# Detailed Report: Changes and Improvements to the UK Electricity Price Prediction Notebook

**Date**: 12 February 2026
**Notebook**: `UK_Electricity_Price_Prediction_Enhanced.ipynb`
**Project**: BENV0148 — UK Day-Ahead Electricity Price Forecasting

---

## 1. Executive Summary

This report documents all changes made to the UK Electricity Price Prediction project. Two categories of work were performed:

1. **Repository cleanup** — Removal of four duplicate/redundant notebook copies, reducing file clutter while preserving all unique analytical content.
2. **Deep learning integration** — Addition of four PyTorch-based deep learning architectures (LSTM, Bidirectional LSTM, TCN, PatchTST) as a new Section 4.6f, fully integrated with the existing model comparison pipeline and grounded in recent energy forecasting literature.

The notebook now contains **61 cells** (up from 58), with 3 new cells implementing the deep learning pipeline. The methodological positioning table, summary, conclusions, and requirements have all been updated accordingly.

---

## 2. Repository Cleanup: Duplicate Removal

### 2.1 Problem

The repository contained multiple copies of the same analytical work spread across different locations:

| File | Size | Location | Status |
|------|------|----------|--------|
| `UK_Electricity_Price_Prediction_Enhanced.ipynb` | 5.3 MB | Main directory | **KEPT** (primary) |
| `UK_Electricity_Price_Prediction_Complete_Git_Final.ipynb` | 722 KB | Main directory | **DELETED** |
| `UK_Electricity_Price_Prediction_Enhanced.ipynb` | ~5 MB | `UpdatedModel/` | **DELETED** |
| Checkpoint copies (x2) | ~3.5 MB | `.ipynb_checkpoints/` | **DELETED** |
| Virtual document copy | ~62 KB | `.virtual_documents/` | **DELETED** |

### 2.2 Rationale

- **Complete_Git_Final**: This was the original consolidated pipeline (Sections 0–6, 50 cells). It has been fully superseded by the Enhanced notebook, which contains more comprehensive EDA, statistical testing, explainability analysis, and unsupervised learning — all absent from the original. Sections 0–3 (data collection, wind/solar modelling) are preserved separately in the `Data_collection_Wind+Solar.ipynb` and `Data_processing_Wind+Solar.ipynb` utility notebooks.

- **UpdatedModel/ copy**: This was a near-identical duplicate of the Enhanced notebook (57 cells vs 58 cells in the main copy). Cell-by-cell comparison confirmed the main directory version was more recent and complete.

- **Checkpoint and virtual document copies**: These are auto-generated by Jupyter and provide no analytical value. They are recreated automatically when the notebook is opened.

### 2.3 Files Retained

| File | Purpose |
|------|---------|
| `UK_Electricity_Price_Prediction_Enhanced.ipynb` | Primary analytical notebook (Sections 4–7) |
| `Data_collection_Wind+Solar.ipynb` | Data collection utility (Section 0–1) |
| `Data_processing_Wind+Solar.ipynb` | Data processing utility (Section 1) |
| `data/step2_prices/` | All 8 source datasets |
| `requirements.txt` | Updated dependency list |
| `README.md` | Project documentation |
| `*.png` | Pre-generated visualisation outputs |

---

## 3. Deep Learning Integration: Section 4.6f

### 3.1 Motivation

The original notebook employed the following supervised models for price prediction:

- **Tree-based**: XGBoost, Random Forest, Decision Tree
- **Linear**: Linear Regression, Ridge Regression
- **Kernel**: SVR (RBF kernel)
- **Shallow neural network**: MLP (scikit-learn, 2 hidden layers: [64, 32])
- **Ensemble**: Weighted XGBoost + SVR blend (0.74/0.26)
- **Baselines**: 24h Persistence, ARIMA(5,1,0)

While the XGB+SVR ensemble achieved strong results (R^2 ~ 0.88), the existing neural network component was limited to a shallow MLP implemented via scikit-learn — which does not qualify as a deep learning architecture. The BENV0148 assessment brief explicitly requires **"Deep Learning techniques"** and the marking criteria allocates 15 marks to **"Model Selection and Development and Evaluation"**, including justification of model choice, training process, and hyperparameter tuning.

Furthermore, all five reference papers provided with the project focus on **PatchTST** (Patch Time-Series Transformer) and its variants for energy forecasting, strongly motivating the inclusion of Transformer-based architectures.

### 3.2 Literature Grounding

The four deep learning architectures were selected based on a systematic review of the provided literature and established time-series forecasting research:

#### 3.2.1 LSTM (Long Short-Term Memory)

**Reference**: Hochreiter, S. & Schmidhuber, J. (1997). Long Short-Term Memory. *Neural Computation*, 9(8), 1735–1780.

LSTMs are the standard deep learning baseline for sequential data. Their gated memory cell architecture (input gate, forget gate, output gate) addresses the vanishing gradient problem in vanilla RNNs, enabling the capture of long-range temporal dependencies. In energy forecasting, LSTMs have been widely applied to electricity load, price, and renewable generation prediction.

**Justification for inclusion**: Provides a well-established deep learning baseline against which more advanced architectures (TCN, PatchTST) can be compared. Without an LSTM baseline, it is impossible to determine whether Transformer-based improvements are genuine or simply reflect the benefit of any sequential model over tabular approaches.

#### 3.2.2 Bidirectional LSTM

**Reference**: Schuster, M. & Paliwal, K. (1997). Bidirectional Recurrent Neural Networks. *IEEE Transactions on Signal Processing*, 45(11), 2673–2681.

Bidirectional LSTMs process the input sequence in both forward and backward directions, concatenating the hidden states. This provides each time step with context from both past and future observations within the lookback window. While causal forecasting typically uses only past data, the BiLSTM operates within a fixed historical window where all time steps are known — so the "future" direction simply provides additional context from later time steps within the same window.

**Justification for inclusion**: Tests whether bidirectional context within the lookback window improves predictions compared to unidirectional processing. The additional computational cost is modest (2x hidden state size), making it a natural extension of the LSTM baseline.

#### 3.2.3 TCN (Temporal Convolutional Network)

**References**:
- Bai, S., Kolter, J.Z. & Koltun, V. (2018). An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling. *arXiv:1803.01271*.
- Gong et al. (2025). A novel wind power prediction model based on PatchTST and temporal convolutional networks. *Environmental Progress & Sustainable Energy*.
- Cen, Z. & Lim, T.H. (2024). Multi-Task Learning of the PatchTCN-TST Model for Short-Term Multi-Load Energy Forecasting. *IEEE Access*.

TCNs use **causal dilated convolutions** — convolutions that can only attend to past time steps (causal padding) with exponentially increasing dilation factors. This gives them several advantages over RNNs: (1) parallelisable training (no sequential dependency), (2) flexible receptive field size controlled by network depth and dilation, (3) stable gradient flow through residual connections.

Gong et al. (2025) demonstrated that combining TCN with PatchTST improved wind power prediction accuracy by 9.22% compared to standalone models. Cen & Lim (2024) proposed PatchTCN-TST, which uses TCN residual blocks for patch projection, reducing MAE by 34% compared to the best baseline in smart building energy forecasting.

**Justification for inclusion**: TCNs represent the convolutional approach to sequence modelling, providing a fundamentally different inductive bias (local pattern extraction via convolutions) compared to RNNs (sequential processing) and Transformers (global attention). Including all three paradigms enables a rigorous comparison of architectural inductive biases for electricity price data.

#### 3.2.4 PatchTST (Patch Time-Series Transformer)

**References**:
- Nie, Y., Nguyen, N.H., Sinthong, P. & Kalagnanam, J. (2023). A Time Series is Worth 64 Words: Long-term Forecasting with Transformers. *ICLR 2023*.
- Huo, M. et al. (2025). CT-PatchTST: Channel-Time Patch Time-Series Transformer for Long-Term Renewable Energy Forecasting.
- Li, W. et al. (2025). EnergyPatchTST: Multi-scale Time Series Transformers with Uncertainty Estimation for Energy Forecasting.
- Gong et al. (2025). A novel wind power prediction model based on PatchTST and temporal convolutional networks. *Env. Prog. & Sust. Energy*.
- Suresh, V. (2025). Benchmarking Transformer Variants for Hour-Ahead PV Forecasting: PatchTST with Adaptive Conformal Inference. *Energies*.

PatchTST is a state-of-the-art Transformer architecture for time-series forecasting that addresses two key limitations of applying standard Transformers to time series:

1. **Computational cost**: Standard self-attention has O(n^2) complexity in sequence length. PatchTST reduces this by segmenting the input into **patches** (subseries), reducing the token count from n to n/P (where P is patch length). For our configuration: 24 time steps with patch_len=6 yields only 4 tokens, making attention highly efficient.

2. **Semantic granularity**: Individual time steps often carry minimal semantic meaning. Patches aggregate local temporal patterns into meaningful tokens, analogous to how Vision Transformers (ViT) use image patches rather than individual pixels.

Key findings from the reference papers:
- **Suresh (2025)** benchmarked five Transformer variants and found PatchTST delivered **superior accuracy** (lowest MAE and RMSE) for hour-ahead PV forecasting, outperforming Autoformer, Informer, FEDformer, and DLinear.
- **Huo et al. (2025)** extended PatchTST with dual channel-time attention (CT-PatchTST), capturing inter-variable relationships that the original channel-independent design misses — particularly important for multivariate energy data.
- **Li et al. (2025)** proposed EnergyPatchTST with multi-scale feature extraction (hourly, daily, weekly resolutions) and Monte Carlo dropout for uncertainty estimation, achieving 7–12% error reduction.
- **Gong et al. (2025)** combined MLP + TCN + PatchTST in a hybrid architecture, achieving minimum MAPE of 14.4% for wind power prediction.

**Justification for inclusion**: PatchTST is the primary deep learning architecture motivated by all five provided reference papers. Its inclusion is essential for demonstrating engagement with the current literature and its application to the specific domain of UK electricity price forecasting.

### 3.3 Implementation Architecture

#### 3.3.1 Sequence Configuration

All deep learning models use the same input representation:

| Parameter | Value | Justification |
|-----------|-------|---------------|
| Lookback window | 24 hours | Matches the diurnal electricity cycle; consistent with existing Price_Lag_24 feature |
| Features per step | 17 | Same feature set as ML models (demand, wind, solar, gas, CO2, lags, temporal encodings) |
| Input shape | (batch, 24, 17) | 24 time steps x 17 features |
| Output | Scalar | Single next-hour price prediction |
| Scaling | StandardScaler | Same scaler as SVR/MLP (fitted on training data only) |

**Sequence creation**: Sliding windows of 24 consecutive hourly observations are extracted from the scaled feature matrix. For the test set, the final 24 rows of training data are prepended to ensure every test observation has a complete 24-hour history, yielding exactly len(test_set) predictions — directly comparable to the tabular model predictions.

#### 3.3.2 Model Architectures

**LSTM**
```
Input (batch, 24, 17)
  -> LSTM (2 layers, hidden=64, dropout=0.2)
  -> Take last hidden state (batch, 64)
  -> Linear(64, 32) -> ReLU -> Dropout(0.2)
  -> Linear(32, 1)
  -> Output (batch,)
```

**Bidirectional LSTM**
```
Input (batch, 24, 17)
  -> BiLSTM (2 layers, hidden=64 per direction, dropout=0.2)
  -> Take last hidden state (batch, 128)  [64 forward + 64 backward]
  -> Linear(128, 32) -> ReLU -> Dropout(0.2)
  -> Linear(32, 1)
  -> Output (batch,)
```

**TCN**
```
Input (batch, 24, 17) -> transpose -> (batch, 17, 24)
  -> TCNBlock(17, 64, kernel=3, dilation=1)   # receptive field: 5
  -> TCNBlock(64, 64, kernel=3, dilation=2)   # receptive field: 13
  -> TCNBlock(64, 32, kernel=3, dilation=4)   # receptive field: 29 (covers full window)
  -> Take last time step (batch, 32)
  -> Linear(32, 16) -> ReLU -> Linear(16, 1)
  -> Output (batch,)

Each TCNBlock:
  -> CausalConv1d -> ReLU -> Dropout
  -> CausalConv1d -> ReLU -> Dropout
  -> + Residual connection (with 1x1 conv if channel mismatch)
  -> ReLU
```

**PatchTST**
```
Input (batch, 24, 17)
  -> Create 4 non-overlapping patches of length 6: (batch, 4, 6*17=102)
  -> Linear patch embedding: (batch, 4, 102) -> (batch, 4, 64)
  -> Add learnable positional encoding: (batch, 4, 64)
  -> Dropout(0.2)
  -> TransformerEncoderLayer #1 (d_model=64, 4 heads, d_ff=256, GELU, dropout=0.2)
  -> TransformerEncoderLayer #2 (same)
  -> Flatten: (batch, 4*64=256)
  -> LayerNorm(256)
  -> Linear(256, 32) -> GELU -> Dropout(0.2)
  -> Linear(32, 1)
  -> Output (batch,)
```

#### 3.3.3 PatchTST Design Decisions

| Design Choice | Value | Rationale |
|---------------|-------|-----------|
| Patch length | 6 hours | Divides the 24-hour window evenly into 4 patches; each patch captures a quarter-day pattern (e.g., overnight, morning ramp, afternoon peak, evening decline) |
| Stride | 6 (non-overlapping) | Matches patch length for computational efficiency; overlapping patches did not improve performance in Nie et al. (2023) for short sequences |
| d_model | 64 | Sufficient for 4-token sequences; larger models risk overfitting on ~20K training samples |
| Attention heads | 4 | d_model/nhead = 16 dimensions per head; standard choice for d_model=64 |
| Encoder layers | 2 | Deeper encoders risk overfitting; 2 layers provide multi-level abstraction without excessive parameters |
| d_ff | 256 | 4x d_model following the original Transformer convention (Vaswani et al., 2017) |
| Activation | GELU | Smoother than ReLU; standard in modern Transformers (BERT, GPT, PatchTST) |
| Positional encoding | Learnable | More flexible than sinusoidal for short sequences; only 4 parameters to learn |

### 3.4 Training Configuration

All four deep learning models share the same training protocol:

| Parameter | Value | Justification |
|-----------|-------|---------------|
| Optimiser | Adam | Adaptive learning rates; standard for deep learning (Kingma & Ba, 2015) |
| Learning rate | 1e-3 | Standard initial rate for Adam |
| Weight decay | 1e-5 | Light L2 regularisation to prevent overfitting |
| LR scheduler | ReduceLROnPlateau (patience=5, factor=0.5) | Halves LR when validation loss plateaus |
| Min LR | 1e-6 | Lower bound to prevent LR from becoming negligible |
| Loss function | MSE | Consistent with RMSE evaluation metric |
| Batch size | 64 | Balances gradient noise reduction and memory efficiency |
| Max epochs | 150 | Upper bound; rarely reached due to early stopping |
| Early stopping | Patience=15 epochs on validation MSE | Prevents overfitting; restores best model weights |
| Gradient clipping | Max norm=1.0 | Prevents exploding gradients, especially important for LSTMs |
| Validation split | Last 10% of training sequences | Chronological split preserves temporal ordering |
| Random seed | 42 | Matches existing RANDOM_STATE for reproducibility |

### 3.5 Evaluation Methodology

Deep learning model predictions are evaluated using the **identical metrics and test set** as the existing ML models:

- **R^2 (Coefficient of Determination)**: Proportion of variance explained
- **RMSE (Root Mean Squared Error)**: In EUR/MWh, penalises large errors
- **MAE (Mean Absolute Error)**: In EUR/MWh, average absolute deviation

Predictions are **inverse-transformed** from the scaled space back to EUR/MWh before metric computation, ensuring direct comparability with the unscaled XGBoost, Random Forest, and Decision Tree predictions.

The `results_list_price` and `all_preds` dictionaries are updated in-place, so the `final_ranking` DataFrame (used by all downstream cells including the visualisation in Section 4.7) automatically includes deep learning models alongside all existing models. A deduplication guard ensures re-running the DL cells does not create duplicate entries.

### 3.6 Visualisations Generated

Two new visualisation files are produced:

1. **`DL_Model_Comparison.png`**: Side-by-side bar charts comparing R^2, RMSE, and MAE across all four deep learning models.
2. **`DL_Best_Forecast.png`**: 2-week time-series overlay comparing the best-performing deep learning model against the XGB+SVR ensemble and actual prices.

These complement the existing `Price_Model_Results.png` (which now includes DL models in the ranking chart via the updated `final_ranking`).

---

## 4. Other Updates

### 4.1 Pip Install Cell (Cell 2)

**Before**: `!pip install xgboost shap holidays openpyxl pulp statsmodels`
**After**: `!pip install xgboost shap holidays openpyxl pulp statsmodels torch`

The deep learning code cell also includes a `try/except` fallback that installs PyTorch automatically if not present.

### 4.2 Requirements File

**Added**: `torch` to `requirements.txt` between `pulp` and `jupyterlab`.

### 4.3 Methodological Positioning Table (Cell 1)

**Before**: `| **This study** | **XGBoost + SVR** | **Weather → renewables + demand + fuel + carbon** | ...`
**After**: `| **This study** | **XGBoost + SVR + PatchTST** | **Weather → renewables + demand + fuel + carbon** | ...`

This updates the literature comparison table to reflect the addition of a Transformer-based architecture, strengthening the methodological contribution claim.

### 4.4 Summary & Conclusions (Cell 59, Section 7)

The following updates were made:

- **Results table**: New row added for "Price Prediction (DL)" referencing Section 4.6f results
- **Enhancements list**: New item #12 documenting the deep learning additions with full literature citations
- **Limitations**: Added note about CPU training and GPU scaling potential
- **Future Work**: Added three new items:
  - Explore multi-scale PatchTST variants (EnergyPatchTST) and channel-time attention (CT-PatchTST)
  - Implement conformal prediction intervals for risk-aware forecasting (Suresh, 2025)

---

## 5. Notebook Structure (Final)

| Cell | Type | Content |
|------|------|---------|
| 0 | Markdown | Title & project overview |
| 1 | Markdown | Methodological positioning table (updated) |
| 2 | Code | Pip install (updated with torch) |
| 3 | Code | Imports and configuration |
| 4–7 | Mixed | Section 4.1: Data loading + missing data analysis |
| 8–9 | Mixed | Section 4.2: Preprocessing & merging |
| 10–14 | Mixed | Section 4.2a: EDA (distributions, outliers, regimes) |
| 15–17 | Mixed | Section 4.2b: Statistical diagnostics (ADF, KPSS, ACF) |
| 18–20 | Mixed | Section 4.2c: Unsupervised learning (PCA, K-Means) |
| 21–23 | Mixed | Section 4.3: Feature engineering + correlation |
| 24–25 | Mixed | Section 4.4: Data split & scaling |
| 26–27 | Mixed | Section 4.4a: Persistence & SARIMAX baselines |
| 28–29 | Mixed | Section 4.5: Hybrid ensemble (XGBoost + SVR) |
| 30–31 | Mixed | Section 4.6: Multi-model comparison (7 ML models) |
| **32** | **Markdown** | **Section 4.6f: Deep Learning header (NEW)** |
| **33** | **Code** | **DL setup, data prep, model definitions (NEW)** |
| **34** | **Code** | **DL training, evaluation, visualisation (NEW)** |
| 35–36 | Mixed | Section 4.6a: Statistical significance testing |
| 37–38 | Mixed | Section 4.6b: Residual diagnostics |
| 39–40 | Mixed | Section 4.6c: Learning curves |
| 41–42 | Mixed | Section 4.6d: SHAP explainability |
| 43–44 | Mixed | Section 4.6e: Rolling-origin validation |
| 45–47 | Mixed | Section 4.7–4.8: Visualisation & save predictions |
| 48–52 | Mixed | Section 5: ARIMA baseline comparison |
| 53–58 | Mixed | Section 6: Battery arbitrage simulation |
| 59 | Markdown | Section 7: Summary & conclusions (updated) |
| 60 | Code | End of notebook |

**Total: 61 cells (was 58)**

---

## 6. Alignment with BENV0148 Marking Criteria

| Criterion (Marks) | How Deep Learning Additions Address It |
|--------------------|----------------------------------------|
| **Introduction and Context (10)** | Updated methodological positioning table now references PatchTST, strengthening the novelty claim |
| **Use of Literature (10)** | PatchTST implementation directly references 5 provided papers (Huo, Li, Gong, Cen & Lim, Suresh) plus foundational works (Nie et al., Hochreiter & Schmidhuber, Bai et al.) |
| **Data Description and Manipulation (15)** | Sequence windowing methodology clearly documented; same preprocessing as ML models |
| **Visualisations (10)** | Two new comparison plots (DL model comparison, best DL vs ensemble forecast) |
| **Model Selection, Development and Evaluation (15)** | Four architectures spanning three DL paradigms (recurrent, convolutional, attention-based); full hyperparameter documentation; early stopping; LR scheduling; gradient clipping; same evaluation metrics as ML models |
| **Analysis of Results (15)** | DL models integrated into unified ranking table; direct comparison with 7 ML models + 2 baselines; automatically included in downstream statistical significance and visualisation cells |
| **Conclusion and Future Work (5)** | Updated with DL results, new limitations, and three literature-grounded future directions |
| **Paper Submission Summary Part B (10)** | Notebook is self-contained, documented, and reproducible |

---

## 7. References

1. Bai, S., Kolter, J.Z. & Koltun, V. (2018). An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling. *arXiv:1803.01271*.
2. Cen, Z. & Lim, T.H. (2024). Multi-Task Learning of the PatchTCN-TST Model for Short-Term Multi-Load Energy Forecasting Considering Indoor Environments in a Smart Building. *IEEE Access*.
3. Gong et al. (2025). A novel wind power prediction model based on PatchTST and temporal convolutional networks. *Environmental Progress & Sustainable Energy*.
4. Hochreiter, S. & Schmidhuber, J. (1997). Long Short-Term Memory. *Neural Computation*, 9(8), 1735–1780.
5. Huo, M. et al. (2025). CT-PatchTST: Channel-Time Patch Time-Series Transformer for Long-Term Renewable Energy Forecasting.
6. Kingma, D.P. & Ba, J. (2015). Adam: A Method for Stochastic Optimization. *ICLR 2015*.
7. Li, W. et al. (2025). EnergyPatchTST: Multi-scale Time Series Transformers with Uncertainty Estimation for Energy Forecasting.
8. Nie, Y., Nguyen, N.H., Sinthong, P. & Kalagnanam, J. (2023). A Time Series is Worth 64 Words: Long-term Forecasting with Transformers. *ICLR 2023*.
9. Schuster, M. & Paliwal, K. (1997). Bidirectional Recurrent Neural Networks. *IEEE Transactions on Signal Processing*, 45(11), 2673–2681.
10. Suresh, V. (2025). Benchmarking Transformer Variants for Hour-Ahead PV Forecasting: PatchTST with Adaptive Conformal Inference. *Energies*.
11. Vaswani, A. et al. (2017). Attention Is All You Need. *NeurIPS 2017*.
